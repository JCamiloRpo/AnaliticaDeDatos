# -*- coding: utf-8 -*-
"""Clustering texto.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LCspmdqnK6D68LKCQjl5ErZSYSPQSOGH

# Clustering de Texto 
1. Preparar el texto
2. Aprendizaje
3. Evaluacion
4. Perfilamiento
"""

#Cargamos librerías principales
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Instalación de paquetes para tratamiento de texto
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import SnowballStemmer
from wordcloud import WordCloud

nltk.download('popular')
stopwords.words('spanish')
stemmer = SnowballStemmer('spanish')

"""#1. Preparación del Texto
- Limpiar el texto
- Eliminar stopwords
- Reducción de raíces
- Visualizaciones del texto
- Representación en vector de características
"""

#Cargamos los datos
data = pd.read_csv("Ejemplo-clustering.csv", sep = ",", na_values = "unknown", encoding='latin-1')
data

#Convertimos a minuscula y eliminamos caracteres especiales
def tokenizar(texto):
  tokens = word_tokenize(texto)
  words = [w.lower() for w in tokens if w.isalnum()]
  return words
data['tokens'] = data['texto'].apply(lambda x: tokenizar(x))
data.head()

#Eliminamos stopwords
def limpiar_tokens(lista):
  clean_tokens = lista[:]
  sr = stopwords.words('spanish')
  for token in lista:
    if token in stopwords.words('spanish'):
      clean_tokens.remove(token)
  return clean_tokens

# Limpiamos los tokens
data['sin_stopwords'] = data['tokens'].apply(lambda x: limpiar_tokens(x))
data.head()

#Reducción a la raíz

def stem_tokens(lista):
  lista_stem = []
  for token in lista:
    lista_stem.append(stemmer.stem(token))
  return lista_stem

data['stemming'] = data['sin_stopwords'].apply(lambda x: stem_tokens(x))
data.head()

#Nube de palabras 
lista_palabras = data["stemming"].tolist()
tokens_abstractos = [keyword.strip() for sublista in lista_palabras for keyword in sublista]
wordcloud = WordCloud( max_words=1000,margin=0).generate((" ").join(tokens_abstractos))
plt.figure(figsize=(15,15))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

#Gráfica de palabras mas frecuentes
freq = nltk.FreqDist(tokens_abstractos)
plt.figure(figsize=(8, 8))
freq.plot(20, cumulative=False)

#Representación en vector de características tf*idf

from sklearn.feature_extraction.text import TfidfVectorizer


def dummy_fun(doc):
    return doc

tfidf = TfidfVectorizer(
    analyzer='word',
    tokenizer=dummy_fun,
    preprocessor=dummy_fun,
    token_pattern=None)  


X = tfidf.fit_transform(data['stemming'])
data_tfidf=pd.DataFrame(X.todense(),columns=tfidf.get_feature_names())
data_tfidf

"""#2. Aprendizaje"""

#Método del codo para encontrar la mejor cantidad de clusters 
from sklearn.cluster import KMeans 
ks = range(2, 7) # crear valores del 2 al 10
inertias = []

for k in ks:
    # Crear  modelo
    model = KMeans(n_clusters=k)
    model.fit(data_tfidf)
    inertias.append(model.inertia_)
    
# Graficar cantidad de clusters vs inertias
plt.plot(ks, inertias, '-o')
plt.xlabel('Numero de clusters, k')
plt.ylabel('inertia')
plt.xticks(ks)

from sklearn.cluster import KMeans 
model = KMeans(n_clusters=3, max_iter=100)
model.fit(data_tfidf)

"""#3. Evaluación"""

#Inercia del modelo
print(f'inertia del modelo= {model.inertia_}')

#Evalucaion:Silueta
from sklearn import metrics
sil=metrics.silhouette_score(data_tfidf,model.predict(data_tfidf))
print(f'Indice de Silueta={sil}') #Como la silueta dio mayor a 0.5 se dejan los cluster elegidos

"""#4. Perfilamiento"""

#Centroides de los clusters
centroides=pd.DataFrame(model.cluster_centers_, columns=tfidf.get_feature_names())
centroides.round(0)

#Cluster asignado a cada registro
data['cluster']=model.predict(data_tfidf)
data

"""#Para mejorar el perfilamiento creamos una nube por cada cluster"""

for cluster in data['cluster'].unique():
    data_cluster = data[data['cluster'] == cluster ].reset_index()
    lista_palabras = data_cluster["stemming"].tolist()
    tokens_abstractos = [keyword.strip() for sublista in lista_palabras for keyword in sublista]
    wordcloud = WordCloud( max_words=1000,margin=0).generate((" ").join(tokens_abstractos))
    plt.figure()
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.show()